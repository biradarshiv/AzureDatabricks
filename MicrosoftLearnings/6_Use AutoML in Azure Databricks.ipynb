{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f6beb7a-6947-4253-b87e-05158b4f2a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "AutoML in Azure Databricks simplifies the process of building an effective machine learning model for your data.\n",
    "AutoML is a feature of Azure Databricks that tries multiple algorithms and parameters with your data to train an optimal machine learning model.\n",
    "\n",
    "\n",
    "Azure Databricks has an AutoML feature that automates the process of training and validating models using different algorithms and hyperparameters. AutoML significantly reduces the effort needed to run and track model training experiments.\n",
    "\n",
    "- You start an AutoML experiment, specifying a table in your Azure Databricks workspace as the data source for training and the specific performance metric for which you want to optimize.\n",
    "- The AutoML experiment generates multiple MLflow runs, each producing a notebook with code to preprocess the data before training and validating a model. The trained models are saved as artifacts in the MLflow runs or files in the DBFS store.\n",
    "- The experiment runs are listed in order of performance, with the best performing models shown first. You can explore the notebooks that were generated for each run, choose the model you want to use, and then register and deploy it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6185c3a-c564-46e5-b693-ef5e13c24993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Powershell to get a databricks environment provisoned\n",
    "\n",
    "rm -r mslearn-databricks -f\n",
    "\n",
    "git clone https://github.com/MicrosoftLearning/mslearn-databricks\n",
    "\n",
    "./mslearn-databricks/setup.ps1\n",
    "\n",
    "./mslearn-databricks/setup.ps1 eastus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f4dc626-04f7-46b8-827b-53a60cb260a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1086daa5-c0f1-4564-89f7-6364695936a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Upload training data to a SQL Warehouse\n",
    "\n",
    "- Download the penguins.csv file from https://raw.githubusercontent.com/MicrosoftLearning/mslearn-databricks/main/data/penguins.csv to your local computer, saving it as penguins.csv.\n",
    "- In the Azure Databricks workspace portal, in the sidebar, select (+) New and then select Add or upload data. \n",
    "- In the Add data page, select Create or modify table and upload the penguins.csv file you downloaded to your computer.\n",
    "- In the Create or modify table from file upload page, select the default schema and set the table name to penguins. \n",
    "- Then select Create table.\n",
    "- When the table has been created, review its details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3f661de-1035-4a77-a975-a9beac706652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create an AutoML experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d58c7ac-ed87-4ed9-aa72-45178cf28540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now that you have some data, you can use it with AutoML to train a model.\n",
    "\n",
    "- In the sidebar on the left, select Experiments.\n",
    "- On the Experiments page, find the Classification tile and select Start training.\n",
    "- Configure the AutoML experiment with the following settings:\n",
    "  - Cluster: Select your cluster\n",
    "  - Input training dataset: Browse to the default database and select the penguins table\n",
    "  - Prediction target: Species\n",
    "  - Experiment name: Penguin-classification\n",
    "  - Advanced configuration:\n",
    "    - Evaluation metric: Precision\n",
    "    - Training frameworks: lightgbm, sklearn, xgboost\n",
    "    - Timeout: 5\n",
    "    - Time column for training/validation/testing split: Leave blank\n",
    "    - Positive label: Leave blank\n",
    "    - Intermediate data storage location: MLflow Artifact\n",
    "- Use the Start AutoML button to start the experiment. Close any information dialogs that are displayed.\n",
    "- Wait for the experiment to complete. You can view details of the runs that are generated under the Runs tab.\n",
    "- After five minutes, the experiment will end. Refreshing the runs will show the run that resulted in the best performing model (based on the precision metric you selected) at the top of the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ca29ebd-f089-4512-9dfa-e5cba145be8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Deploy the best performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e93db5bf-b73a-4b19-9028-21bd0e00497e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Having run an AutoML experiment, you can explore the best performing model that it generated.\n",
    "\n",
    "- In the Penguin-classification experiment page, select View notebook for best model to open the notebook used to train the model in a new browser tab.\n",
    "- Scroll through the cells in the notebook, noting the code that was used to train the model.\n",
    "- Close the browser tab containing the notebook to return to the Penguin-classification experiment page.\n",
    "- In the list of runs, select the name of the first run (which produced the best model) to open it.\n",
    "- In the Artifacts section, note that the model has been saved as an MLflow artifact. Then use the Register model button to register the model as a new model named Penguin-Classifier.\n",
    "- In the sidebar on the left, switch to the Models page. Then select the Penguin-Classifier model you just registered.\n",
    "- On the Penguin-Classifier page, use the Use model for inference button to create a new real-time endpoint with the following settings:\n",
    "  - Model: Penguin-Classifier\n",
    "  - Model version: 1\n",
    "  - Endpoint: classify-penguin\n",
    "  - Compute size: Small\n",
    "The serving endpoint is hosted in a new cluster, which it may take several minutes to create.\n",
    "\n",
    "- When the endpoint has been created, use the Query endpoint button at the top right to open an interface from which you can test the endpoint. Then in the test interface, on the Browser tab, enter the following JSON request and use the Send Request button to call the endpoint and generate a prediction.\n",
    "\n",
    "code\n",
    " {\n",
    "   \"dataframe_records\": [\n",
    "   {\n",
    "      \"Island\": \"Biscoe\",\n",
    "      \"CulmenLength\": 48.7,\n",
    "      \"CulmenDepth\": 14.1,\n",
    "      \"FlipperLength\": 210,\n",
    "      \"BodyMass\": 4450\n",
    "   }\n",
    "   ]\n",
    " }\n",
    " \n",
    "- Experiment with a few different values for the penguin features and observe the results that are returned. Then, close the test interface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e436dc19-2c73-4bfc-b1e2-30e096a305b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89119511-d3a8-418a-9219-9f0fcc216128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- When the endpoint is not longer required, you should delete it to avoid unnecessary costs.\n",
    "\n",
    "- In the classify-penguin endpoint page, in the ‚Åù menu, select Delete."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "6_Use AutoML in Azure Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}